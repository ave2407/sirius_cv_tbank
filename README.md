# T‑Bank Logo Detection API (tlogo‑detection)

REST‑сервис детекции логотипа **Т‑Банка** (стилизованная «Т» в щите, любой цвет). Логотипы «Тинькофф» считаем **негативами** и игнорируем.

* **Форматы**: JPEG / PNG / BMP / WEBP
* **Скорость**: ≤ 10 с/изображение (целевой SLA)
* **Порт**: `8000`
* **Веса**: `weights/best.onnx` (копируются в образ как `/models/best.onnx`)
* **Инференс**: ONNX Runtime (по умолчанию CPU; тюнинг потоков через ENV)

---

## Быстрый старт (Docker)

Сборка
```bash
docker build -t tlogo-api-cpu:latest -f Dockerfile .
```
Запуск (используются веса, встроенные в образ как /models/best.onnx)
```bash
docker run --rm -it -p 8000:8000 tbank-api-cpu:latest
```

Проверка:

```bash
curl http://127.0.0.1:8000/healthz
```
{"status":"ok"}
```bash
curl.exe -s -X POST "http://127.0.0.1:8000/detect" -F "file=@demo/test_2logo.jpg"
```
{"detections":[{"bbox":{"x_min":...,"y_min":...,"x_max":...,"y_max":...}}]}

ENV по умолчанию (можно переопределить при запуске):

* `MODEL_PATH=/models/best.onnx`
* `CONF_THRES=0.55`
* `IOU_THRES=0.50`
* `IMG_SIZE=960`
* `ORT_INTRA_THREADS=4`, `ORT_INTER_THREADS=1`

---

## API

Реализованы эндпоинты:

* `GET /healthz` — статус
* `POST /detect` — приём изображения, возврат абсолютных координат `bbox` в контракте из `server/schemas.py`

Интерактивная документация: `/docs` (Swagger) и `/redoc` включены. Контракт строго соответствует заданию.

---

## Структура репозитория

```
sirius_cv/
├── server/                    # REST API + инференс
│   ├── app.py                 # маршруты и валидация
│   ├── inference.py           # препроцесс → ORT → постпроцесс (NMS, clip)
│   ├── schemas.py             # Pydantic модели ответа/ошибок
│   └── utils.py               # letterbox, NMS, преобразования
├── data/
│   ├── merged_tlogo/          # итоговый YOLO‑датасет (train/valid/test)
│   ├── interim/auto_owlvit/   # автолейблы OWLv2 (images/labels)
│   ├── annotations/{coco,yolo}
│   ├── raw/  sample/  sample2/
├── training/
│   ├── configs/               # (yaml и пр.)
│   └── scripts/
│       ├── 00_sample_random.py        # отбор случайных изображений
│       ├── 01_auto_annotate.py        # zero‑shot автолейблинг (OWLv2)
│       ├── 02_download_rf_dataset.py  # загрузка датасетов Roboflow
│       └── 02_find_iou.py             # свип IoU/NMS на валидации
├── docker/Dockerfile
├── weights/                   # best.onnx (не хранится в гите)
└── test.py                    # утилита для экспорта дерева и содержимого проекта
```

---

## Данные (кратко)

* **Орг‑датасет**: архив с неразмеченными изображениями (см. инструкцию в ноутбуке/скриптах). Парольный архив распаковывался в `data/raw`.
* **Zero‑shot автолейблинг**: `training/scripts/01_auto_annotate.py` (OWLv2) на части сырых данных → YOLO‑txt в `data/interim/auto_owlvit`.
* **Негативные примеры**: кадры без T‑Bank, в т.ч. с логотипом «Тинькофф», включались как **пустые** файлы разметки (hard negatives).
* **Сводный набор**: `data/merged_tlogo/{train,valid,test}` (YOLO‑формат). Сплиты фиксированы.

---

## Как я обучал модель (подробно)

Ниже — реальный путь, по которому шёл, и почему принимались решения.

1. **Старт: 500 ручных аннотаций в Roboflow**
   Для ускорения старта пометил \~500 изображений с критериями класса: только «Т в щите», цвет любой. Логотипы «Тинькофф» оставлялись **без** боксов (как негативы).

2. **Первая модель в Roboflow (YOLOv11 Fast)**
   Обучил быструю конфигурацию прямо в RF для быстрого цикла обратной связи. На их валидации: *mAP\@50 ≈ 94%*, *Precision ≈ 96.8%*, *Recall ≈ 89.9%*. Эту модель использовал как teacher для полуавторазметки.

3. **Авторазметка ещё ≈500 изображений**
   С помощью RF‑модели и локального `01_auto_annotate.py` (OWLv2) добрал ещё \~500 аннотаций. Ручная чистка ложно‑положительных на «Тинькофф».

4. **Попытка локального обучения YOLOv11s в Colab**
   Запустил обучение на суммарно \~1k примерах. Результат оказался хуже RF‑модели: **низкий Recall** на мелких/ракурсных логотипах. Пробовал **оверсемплинг** позитивов и hard negatives — частично помогало, но метрики всё равно колебались и переобучались на «типичные» сцены.

5. **Переосмысление датасета: нашёл больше данных в Roboflow**
   В RF есть поисковые датасеты/версии. Я расширил исходный набор до **\~8k изображений** (включая разные сцены/ракурсы/масштабы). Это резко улучшило покрытие редких кейсов.

6. **Балансировка: даунсемплинг train**
   После слияния доля «простых» позитивов стала завышенной, что ухудшало точность на фоне. Я **даунсемплировал train**, чтобы снизить смещение и ускорить итерации. `val/test` не трогал для честной оценки.

7. **Смена модели на YOLOv11m**
   Увеличил ёмкость модели с `11s` → `11m`, что дало **лучший Recall** на мелких объектах без критичного падения скорости. Итоговую модель экспортировал в **ONNX** (`weights/best.onnx`).

8. **Подбор постпроцесса (NMS/пороги)**
   Скрипт `training/scripts/02_find_iou.py` использовал для свипа **IoU NMS** на валидации и подбора порогов. Итоговые параметры в окружении по умолчанию: `CONF_THRES=0.55`, `IOU_THRES=0.50`, `IMG_SIZE=896`.

Итог: на расширенном датасете + `yolov11m` достигнут устойчивый баланс P/R, заметно ниже ложноположительных на «Тинькофф», уверенная работа на разных цветах логотипа.

---

## Валидация и метрики

* Сводные сплиты лежат в `data/merged_tlogo/valid` и `.../test`.
* Для анализа чувствительности к NMS/IoU используйте `training/scripts/02_find_iou.py` — он строит CSV и HTML‑графики (PR и mAP по IoU).
* Валидационные визуализации и табличные метрики прикладывайте к релизу/отчёту (скриншоты добавляются в `out/` по желанию).

> Примечание: конкретные числа по открытой валидации зависят от версии набора и порогов; в репозитории зафиксированы конфиги и скрипты для воспроизводимости.

---

## Производительность

* Инференс на **ONNX Runtime (CPU)** с тюнингом потоков (`ORT_INTRA_THREADS`, `ORT_INTER_THREADS`).
* Препроцесс: `letterbox` до квадрата `IMG_SIZE` без искажения аспекта.
* Постпроцесс: NMS на NumPy, клип координат, ограничение `MAX_DET`.

При наличии GPU можно собрать альтернативный образ с `onnxruntime-gpu` и включить CUDA‑провайдер (минимальные правки в `server/inference.py`).

---

## Известные ограничения

* Очень маленькие логотипы (< \~8×8 px) и сильное размытие ухудшают Recall.
* Экстремальные повороты/перевороты встречаются редко и покрыты хуже.

---

## Планы улучшений

* Экспорт в TensorRT (FP16/INT8) и GPU‑NMS.
* Более явный hard‑negative mining по «Тинькофф» из внешних источников.
* Мульти‑scale TTA как опция оффлайн‑валидации (не в прод‑API).

---

## Лицензия

MIT (или укажите иную, если требуется).
